{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  37607\n",
      "Valid Size:  16132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_697/3381582398.py:499: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 48\n",
      "Epoch 48/50, Batch 0/2351, Loss: 0.1691\n",
      "Epoch 48/50, Batch 10/2351, Loss: 0.1610\n",
      "Epoch 48/50, Batch 20/2351, Loss: 0.3050\n",
      "Epoch 48/50, Batch 30/2351, Loss: 0.2949\n",
      "Epoch 48/50, Batch 40/2351, Loss: 0.0169\n",
      "Epoch 48/50, Batch 50/2351, Loss: 0.1400\n",
      "Epoch 48/50, Batch 60/2351, Loss: 0.1868\n",
      "Epoch 48/50, Batch 70/2351, Loss: 0.3003\n",
      "Epoch 48/50, Batch 80/2351, Loss: 0.1714\n",
      "Epoch 48/50, Batch 90/2351, Loss: 0.0051\n",
      "Epoch 48/50, Batch 100/2351, Loss: 0.2022\n",
      "Epoch 48/50, Batch 110/2351, Loss: 0.2211\n",
      "Epoch 48/50, Batch 120/2351, Loss: 0.1219\n",
      "Epoch 48/50, Batch 130/2351, Loss: 0.0610\n",
      "Epoch 48/50, Batch 140/2351, Loss: 0.2481\n",
      "Epoch 48/50, Batch 150/2351, Loss: 0.2734\n",
      "Epoch 48/50, Batch 160/2351, Loss: 0.2036\n",
      "Epoch 48/50, Batch 170/2351, Loss: 0.1547\n",
      "Epoch 48/50, Batch 180/2351, Loss: 0.1810\n",
      "Epoch 48/50, Batch 190/2351, Loss: 0.1427\n",
      "Epoch 48/50, Batch 200/2351, Loss: 0.0574\n",
      "Epoch 48/50, Batch 210/2351, Loss: 0.0022\n",
      "Epoch 48/50, Batch 220/2351, Loss: 0.3675\n",
      "Epoch 48/50, Batch 230/2351, Loss: 0.0230\n",
      "Epoch 48/50, Batch 240/2351, Loss: 0.0535\n",
      "Epoch 48/50, Batch 250/2351, Loss: 0.1718\n",
      "Epoch 48/50, Batch 260/2351, Loss: 0.2336\n",
      "Epoch 48/50, Batch 270/2351, Loss: 0.2419\n",
      "Epoch 48/50, Batch 280/2351, Loss: 0.2705\n",
      "Epoch 48/50, Batch 290/2351, Loss: 0.1906\n",
      "Epoch 48/50, Batch 300/2351, Loss: 0.1047\n",
      "Epoch 48/50, Batch 310/2351, Loss: 0.0290\n",
      "Epoch 48/50, Batch 320/2351, Loss: 0.0085\n",
      "Epoch 48/50, Batch 330/2351, Loss: 0.2562\n",
      "Epoch 48/50, Batch 340/2351, Loss: 0.2226\n",
      "Epoch 48/50, Batch 350/2351, Loss: 0.0527\n",
      "Epoch 48/50, Batch 360/2351, Loss: 0.2243\n",
      "Epoch 48/50, Batch 370/2351, Loss: 0.2464\n",
      "Epoch 48/50, Batch 380/2351, Loss: 0.0563\n",
      "Epoch 48/50, Batch 390/2351, Loss: 0.4469\n",
      "Epoch 48/50, Batch 400/2351, Loss: 0.0950\n",
      "Epoch 48/50, Batch 410/2351, Loss: 0.2268\n",
      "Epoch 48/50, Batch 420/2351, Loss: 0.2704\n",
      "Epoch 48/50, Batch 430/2351, Loss: 0.3818\n",
      "Epoch 48/50, Batch 440/2351, Loss: 0.4246\n",
      "Epoch 48/50, Batch 450/2351, Loss: 0.2254\n",
      "Epoch 48/50, Batch 460/2351, Loss: 0.0280\n",
      "Epoch 48/50, Batch 470/2351, Loss: 0.7631\n",
      "Epoch 48/50, Batch 480/2351, Loss: 0.0139\n",
      "Epoch 48/50, Batch 490/2351, Loss: 0.0689\n",
      "Epoch 48/50, Batch 500/2351, Loss: 0.4664\n",
      "Epoch 48/50, Batch 510/2351, Loss: 0.2309\n",
      "Epoch 48/50, Batch 520/2351, Loss: 0.1843\n",
      "Epoch 48/50, Batch 530/2351, Loss: 0.0670\n",
      "Epoch 48/50, Batch 540/2351, Loss: 0.2332\n",
      "Epoch 48/50, Batch 550/2351, Loss: 0.0817\n",
      "Epoch 48/50, Batch 560/2351, Loss: 0.1406\n",
      "Epoch 48/50, Batch 570/2351, Loss: 0.3679\n",
      "Epoch 48/50, Batch 580/2351, Loss: 0.0888\n",
      "Epoch 48/50, Batch 590/2351, Loss: 0.0912\n",
      "Epoch 48/50, Batch 600/2351, Loss: 0.2911\n",
      "Epoch 48/50, Batch 610/2351, Loss: 0.3374\n",
      "Epoch 48/50, Batch 620/2351, Loss: 0.1927\n",
      "Epoch 48/50, Batch 630/2351, Loss: 0.3444\n",
      "Epoch 48/50, Batch 640/2351, Loss: 0.1542\n",
      "Epoch 48/50, Batch 650/2351, Loss: 0.1939\n",
      "Epoch 48/50, Batch 660/2351, Loss: 0.2196\n",
      "Epoch 48/50, Batch 670/2351, Loss: 0.2859\n",
      "Epoch 48/50, Batch 680/2351, Loss: 0.0327\n",
      "Epoch 48/50, Batch 690/2351, Loss: 0.0966\n",
      "Epoch 48/50, Batch 700/2351, Loss: 0.3549\n",
      "Epoch 48/50, Batch 710/2351, Loss: 0.1174\n",
      "Epoch 48/50, Batch 720/2351, Loss: 0.2278\n",
      "Epoch 48/50, Batch 730/2351, Loss: 0.0434\n",
      "Epoch 48/50, Batch 740/2351, Loss: 0.1125\n",
      "Epoch 48/50, Batch 750/2351, Loss: 0.3279\n",
      "Epoch 48/50, Batch 760/2351, Loss: 0.1484\n",
      "Epoch 48/50, Batch 770/2351, Loss: 0.4656\n",
      "Epoch 48/50, Batch 780/2351, Loss: 0.2959\n",
      "Epoch 48/50, Batch 790/2351, Loss: 0.3409\n",
      "Epoch 48/50, Batch 800/2351, Loss: 0.0801\n",
      "Epoch 48/50, Batch 810/2351, Loss: 0.6965\n",
      "Epoch 48/50, Batch 820/2351, Loss: 0.3178\n",
      "Epoch 48/50, Batch 830/2351, Loss: 0.3750\n",
      "Epoch 48/50, Batch 840/2351, Loss: 0.1888\n",
      "Epoch 48/50, Batch 850/2351, Loss: 0.2620\n",
      "Epoch 48/50, Batch 860/2351, Loss: 0.1660\n",
      "Epoch 48/50, Batch 870/2351, Loss: 0.0760\n",
      "Epoch 48/50, Batch 880/2351, Loss: 0.7901\n",
      "Epoch 48/50, Batch 890/2351, Loss: 0.0540\n",
      "Epoch 48/50, Batch 900/2351, Loss: 0.2966\n",
      "Epoch 48/50, Batch 910/2351, Loss: 0.2221\n",
      "Epoch 48/50, Batch 920/2351, Loss: 0.6986\n",
      "Epoch 48/50, Batch 930/2351, Loss: 0.0735\n",
      "Epoch 48/50, Batch 940/2351, Loss: 0.1421\n",
      "Epoch 48/50, Batch 950/2351, Loss: 0.0513\n",
      "Epoch 48/50, Batch 960/2351, Loss: 0.5908\n",
      "Epoch 48/50, Batch 970/2351, Loss: 0.2380\n",
      "Epoch 48/50, Batch 980/2351, Loss: 0.6656\n",
      "Epoch 48/50, Batch 990/2351, Loss: 0.1708\n",
      "Epoch 48/50, Batch 1000/2351, Loss: 0.3346\n",
      "Epoch 48/50, Batch 1010/2351, Loss: 0.0257\n",
      "Epoch 48/50, Batch 1020/2351, Loss: 0.0422\n",
      "Epoch 48/50, Batch 1030/2351, Loss: 0.1442\n",
      "Epoch 48/50, Batch 1040/2351, Loss: 0.2007\n",
      "Epoch 48/50, Batch 1050/2351, Loss: 0.1189\n",
      "Epoch 48/50, Batch 1060/2351, Loss: 0.1380\n",
      "Epoch 48/50, Batch 1070/2351, Loss: 0.2563\n",
      "Epoch 48/50, Batch 1080/2351, Loss: 0.1151\n",
      "Epoch 48/50, Batch 1090/2351, Loss: 0.0564\n",
      "Epoch 48/50, Batch 1100/2351, Loss: 0.4226\n",
      "Epoch 48/50, Batch 1110/2351, Loss: 0.0888\n",
      "Epoch 48/50, Batch 1120/2351, Loss: 0.2618\n",
      "Epoch 48/50, Batch 1130/2351, Loss: 0.2176\n",
      "Epoch 48/50, Batch 1140/2351, Loss: 0.2348\n",
      "Epoch 48/50, Batch 1150/2351, Loss: 0.2152\n",
      "Epoch 48/50, Batch 1160/2351, Loss: 0.1832\n",
      "Epoch 48/50, Batch 1170/2351, Loss: 0.2659\n",
      "Epoch 48/50, Batch 1180/2351, Loss: 0.1559\n",
      "Epoch 48/50, Batch 1190/2351, Loss: 0.2839\n",
      "Epoch 48/50, Batch 1200/2351, Loss: 0.2477\n",
      "Epoch 48/50, Batch 1210/2351, Loss: 0.0598\n",
      "Epoch 48/50, Batch 1220/2351, Loss: 0.0597\n",
      "Epoch 48/50, Batch 1230/2351, Loss: 0.2386\n",
      "Epoch 48/50, Batch 1240/2351, Loss: 0.2519\n",
      "Epoch 48/50, Batch 1250/2351, Loss: 0.0575\n",
      "Epoch 48/50, Batch 1260/2351, Loss: 0.0305\n",
      "Epoch 48/50, Batch 1270/2351, Loss: 0.3373\n",
      "Epoch 48/50, Batch 1280/2351, Loss: 0.0714\n",
      "Epoch 48/50, Batch 1290/2351, Loss: 0.0551\n",
      "Epoch 48/50, Batch 1300/2351, Loss: 0.1499\n",
      "Epoch 48/50, Batch 1310/2351, Loss: 0.0407\n",
      "Epoch 48/50, Batch 1320/2351, Loss: 0.3342\n",
      "Epoch 48/50, Batch 1330/2351, Loss: 0.7804\n",
      "Epoch 48/50, Batch 1340/2351, Loss: 0.1387\n",
      "Epoch 48/50, Batch 1350/2351, Loss: 0.7333\n",
      "Epoch 48/50, Batch 1360/2351, Loss: 0.1105\n",
      "Epoch 48/50, Batch 1370/2351, Loss: 0.2011\n",
      "Epoch 48/50, Batch 1380/2351, Loss: 0.1727\n",
      "Epoch 48/50, Batch 1390/2351, Loss: 0.2659\n",
      "Epoch 48/50, Batch 1400/2351, Loss: 0.0558\n",
      "Epoch 48/50, Batch 1410/2351, Loss: 0.1172\n",
      "Epoch 48/50, Batch 1420/2351, Loss: 0.0271\n",
      "Epoch 48/50, Batch 1430/2351, Loss: 0.3136\n",
      "Epoch 48/50, Batch 1440/2351, Loss: 0.3765\n",
      "Epoch 48/50, Batch 1450/2351, Loss: 0.1856\n",
      "Epoch 48/50, Batch 1460/2351, Loss: 0.1672\n",
      "Epoch 48/50, Batch 1470/2351, Loss: 0.5664\n",
      "Epoch 48/50, Batch 1480/2351, Loss: 0.2800\n",
      "Epoch 48/50, Batch 1490/2351, Loss: 0.2701\n",
      "Epoch 48/50, Batch 1500/2351, Loss: 0.4702\n",
      "Epoch 48/50, Batch 1510/2351, Loss: 0.6506\n",
      "Epoch 48/50, Batch 1520/2351, Loss: 0.0066\n",
      "Epoch 48/50, Batch 1530/2351, Loss: 0.1437\n",
      "Epoch 48/50, Batch 1540/2351, Loss: 0.0698\n",
      "Epoch 48/50, Batch 1550/2351, Loss: 0.2221\n",
      "Epoch 48/50, Batch 1560/2351, Loss: 0.3358\n",
      "Epoch 48/50, Batch 1570/2351, Loss: 0.0686\n",
      "Epoch 48/50, Batch 1580/2351, Loss: 0.2575\n",
      "Epoch 48/50, Batch 1590/2351, Loss: 0.2851\n",
      "Epoch 48/50, Batch 1600/2351, Loss: 0.1310\n",
      "Epoch 48/50, Batch 1610/2351, Loss: 0.1214\n",
      "Epoch 48/50, Batch 1620/2351, Loss: 0.2376\n",
      "Epoch 48/50, Batch 1630/2351, Loss: 0.1600\n",
      "Epoch 48/50, Batch 1640/2351, Loss: 0.0335\n",
      "Epoch 48/50, Batch 1650/2351, Loss: 0.0914\n",
      "Epoch 48/50, Batch 1660/2351, Loss: 0.1028\n",
      "Epoch 48/50, Batch 1670/2351, Loss: 0.0384\n",
      "Epoch 48/50, Batch 1680/2351, Loss: 0.1492\n",
      "Epoch 48/50, Batch 1690/2351, Loss: 0.1972\n",
      "Epoch 48/50, Batch 1700/2351, Loss: 0.2996\n",
      "Epoch 48/50, Batch 1710/2351, Loss: 0.0179\n",
      "Epoch 48/50, Batch 1720/2351, Loss: 0.5002\n",
      "Epoch 48/50, Batch 1730/2351, Loss: 0.3452\n",
      "Epoch 48/50, Batch 1740/2351, Loss: 0.3344\n",
      "Epoch 48/50, Batch 1750/2351, Loss: 0.2578\n",
      "Epoch 48/50, Batch 1760/2351, Loss: 0.1731\n",
      "Epoch 48/50, Batch 1770/2351, Loss: 0.0910\n",
      "Epoch 48/50, Batch 1780/2351, Loss: 0.0132\n",
      "Epoch 48/50, Batch 1790/2351, Loss: 0.2363\n",
      "Epoch 48/50, Batch 1800/2351, Loss: 0.1352\n",
      "Epoch 48/50, Batch 1810/2351, Loss: 0.1186\n",
      "Epoch 48/50, Batch 1820/2351, Loss: 0.3464\n",
      "Epoch 48/50, Batch 1830/2351, Loss: 0.1759\n",
      "Epoch 48/50, Batch 1840/2351, Loss: 0.2938\n",
      "Epoch 48/50, Batch 1850/2351, Loss: 0.0707\n",
      "Epoch 48/50, Batch 1860/2351, Loss: 0.3439\n",
      "Epoch 48/50, Batch 1870/2351, Loss: 0.0472\n",
      "Epoch 48/50, Batch 1880/2351, Loss: 0.0623\n",
      "Epoch 48/50, Batch 1890/2351, Loss: 0.0632\n",
      "Epoch 48/50, Batch 1900/2351, Loss: 0.0743\n",
      "Epoch 48/50, Batch 1910/2351, Loss: 0.2631\n",
      "Epoch 48/50, Batch 1920/2351, Loss: 0.1931\n",
      "Epoch 48/50, Batch 1930/2351, Loss: 0.6346\n",
      "Epoch 48/50, Batch 1940/2351, Loss: 0.0210\n",
      "Epoch 48/50, Batch 1950/2351, Loss: 0.1658\n",
      "Epoch 48/50, Batch 1960/2351, Loss: 0.3841\n",
      "Epoch 48/50, Batch 1970/2351, Loss: 0.2008\n",
      "Epoch 48/50, Batch 1980/2351, Loss: 0.5794\n",
      "Epoch 48/50, Batch 1990/2351, Loss: 0.4696\n",
      "Epoch 48/50, Batch 2000/2351, Loss: 0.2747\n",
      "Epoch 48/50, Batch 2010/2351, Loss: 0.2377\n",
      "Epoch 48/50, Batch 2020/2351, Loss: 0.1791\n",
      "Epoch 48/50, Batch 2030/2351, Loss: 0.1610\n",
      "Epoch 48/50, Batch 2040/2351, Loss: 0.0136\n",
      "Epoch 48/50, Batch 2050/2351, Loss: 0.3564\n",
      "Epoch 48/50, Batch 2060/2351, Loss: 0.1455\n",
      "Epoch 48/50, Batch 2070/2351, Loss: 0.1257\n",
      "Epoch 48/50, Batch 2080/2351, Loss: 0.4224\n",
      "Epoch 48/50, Batch 2090/2351, Loss: 0.2323\n",
      "Epoch 48/50, Batch 2100/2351, Loss: 0.1314\n",
      "Epoch 48/50, Batch 2110/2351, Loss: 0.2278\n",
      "Epoch 48/50, Batch 2120/2351, Loss: 0.6387\n",
      "Epoch 48/50, Batch 2130/2351, Loss: 0.1221\n",
      "Epoch 48/50, Batch 2140/2351, Loss: 0.1404\n",
      "Epoch 48/50, Batch 2150/2351, Loss: 0.2210\n",
      "Epoch 48/50, Batch 2160/2351, Loss: 0.0383\n",
      "Epoch 48/50, Batch 2170/2351, Loss: 0.0854\n",
      "Epoch 48/50, Batch 2180/2351, Loss: 0.0516\n",
      "Epoch 48/50, Batch 2190/2351, Loss: 0.3032\n",
      "Epoch 48/50, Batch 2200/2351, Loss: 0.6230\n",
      "Epoch 48/50, Batch 2210/2351, Loss: 0.1245\n",
      "Epoch 48/50, Batch 2220/2351, Loss: 0.1100\n",
      "Epoch 48/50, Batch 2230/2351, Loss: 0.2160\n",
      "Epoch 48/50, Batch 2240/2351, Loss: 0.2121\n",
      "Epoch 48/50, Batch 2250/2351, Loss: 0.4079\n",
      "Epoch 48/50, Batch 2260/2351, Loss: 0.0588\n",
      "Epoch 48/50, Batch 2270/2351, Loss: 0.1486\n",
      "Epoch 48/50, Batch 2280/2351, Loss: 0.2164\n",
      "Epoch 48/50, Batch 2290/2351, Loss: 0.1365\n",
      "Epoch 48/50, Batch 2300/2351, Loss: 0.3159\n",
      "Epoch 48/50, Batch 2310/2351, Loss: 0.0086\n",
      "Epoch 48/50, Batch 2320/2351, Loss: 0.1583\n",
      "Epoch 48/50, Batch 2330/2351, Loss: 0.0314\n",
      "Epoch 48/50, Batch 2340/2351, Loss: 0.1952\n",
      "Epoch 48/50, Batch 2350/2351, Loss: 0.5622\n",
      "Epoch 48/50, Loss: 0.2124, Accuracy: 0.9251\n",
      "Checkpoint saved at epoch 48\n",
      "Validation Loss: 0.2948, Accuracy: 0.9058\n",
      "Specificity:  0.9786885022458109\n",
      "Recall:  0.9058393255640962\n",
      "Balanced Accuracy:  0.9422639139049536\n",
      "auc : 0.8538, acc : 0.9058, precision : 0.8996, recall : 0.9058, f1_score : 0.9009, specificity : 0.9787, balanced_acc : 0.9423\n",
      "------------------------------------ gt vs. pred ------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8072    0.7243    0.7635       497\n",
      "           1     0.8955    0.7159    0.7957       359\n",
      "           2     0.6426    0.6710    0.6565      1155\n",
      "           3     0.5374    0.2660    0.3559       297\n",
      "           4     0.9321    0.7676    0.8419       340\n",
      "           5     0.7389    0.8251    0.7796       343\n",
      "           6     0.9536    0.9811    0.9672     12287\n",
      "           7     0.5517    0.4160    0.4743       500\n",
      "           8     0.9210    0.9371    0.9289       286\n",
      "           9     0.9054    0.9853    0.9437        68\n",
      "\n",
      "    accuracy                         0.9058     16132\n",
      "   macro avg     0.7885    0.7289    0.7507     16132\n",
      "weighted avg     0.8996    0.9058    0.9009     16132\n",
      "\n",
      "[[  360     9    71     3     0     1    44     9     0     0]\n",
      " [    5   257    47    10     0     5    30     5     0     0]\n",
      " [   51    13   775    26     5    14   196    60    15     0]\n",
      " [    6     0   115    79     1     4    52    39     1     0]\n",
      " [    4     0    28     1   261    10    33     3     0     0]\n",
      " [    4     0     7     0     1   283    43     5     0     0]\n",
      " [    8     0    97     9    11    52 12055    48     6     1]\n",
      " [    6     8    64    19     1    14   180   208     0     0]\n",
      " [    2     0     2     0     0     0     8     0   268     6]\n",
      " [    0     0     0     0     0     0     0     0     1    67]]\n",
      "-------------------------------------------------------------------------------------\n",
      "Accuracy:  0.9058393255640962\n",
      "Epoch 49/50, Batch 0/2351, Loss: 0.0157\n",
      "Epoch 49/50, Batch 10/2351, Loss: 0.0148\n",
      "Epoch 49/50, Batch 20/2351, Loss: 0.2196\n",
      "Epoch 49/50, Batch 30/2351, Loss: 0.1984\n",
      "Epoch 49/50, Batch 40/2351, Loss: 0.0731\n",
      "Epoch 49/50, Batch 50/2351, Loss: 0.3875\n",
      "Epoch 49/50, Batch 60/2351, Loss: 0.2106\n",
      "Epoch 49/50, Batch 70/2351, Loss: 0.4439\n",
      "Epoch 49/50, Batch 80/2351, Loss: 0.0789\n",
      "Epoch 49/50, Batch 90/2351, Loss: 0.3000\n",
      "Epoch 49/50, Batch 100/2351, Loss: 0.5949\n",
      "Epoch 49/50, Batch 110/2351, Loss: 0.1227\n",
      "Epoch 49/50, Batch 120/2351, Loss: 0.1102\n",
      "Epoch 49/50, Batch 130/2351, Loss: 0.1673\n",
      "Epoch 49/50, Batch 140/2351, Loss: 0.2686\n",
      "Epoch 49/50, Batch 150/2351, Loss: 0.0113\n",
      "Epoch 49/50, Batch 160/2351, Loss: 0.0996\n",
      "Epoch 49/50, Batch 170/2351, Loss: 0.0876\n",
      "Epoch 49/50, Batch 180/2351, Loss: 0.0221\n",
      "Epoch 49/50, Batch 190/2351, Loss: 0.0723\n",
      "Epoch 49/50, Batch 200/2351, Loss: 0.0076\n",
      "Epoch 49/50, Batch 210/2351, Loss: 0.1558\n",
      "Epoch 49/50, Batch 220/2351, Loss: 0.0343\n",
      "Epoch 49/50, Batch 230/2351, Loss: 0.1577\n",
      "Epoch 49/50, Batch 240/2351, Loss: 0.2622\n",
      "Epoch 49/50, Batch 250/2351, Loss: 0.1164\n",
      "Epoch 49/50, Batch 260/2351, Loss: 0.1344\n",
      "Epoch 49/50, Batch 270/2351, Loss: 0.0639\n",
      "Epoch 49/50, Batch 280/2351, Loss: 0.1248\n",
      "Epoch 49/50, Batch 290/2351, Loss: 0.0449\n",
      "Epoch 49/50, Batch 300/2351, Loss: 0.4473\n",
      "Epoch 49/50, Batch 310/2351, Loss: 0.1402\n",
      "Epoch 49/50, Batch 320/2351, Loss: 0.2337\n",
      "Epoch 49/50, Batch 330/2351, Loss: 0.2441\n",
      "Epoch 49/50, Batch 340/2351, Loss: 0.1797\n",
      "Epoch 49/50, Batch 350/2351, Loss: 0.2251\n",
      "Epoch 49/50, Batch 360/2351, Loss: 0.3897\n",
      "Epoch 49/50, Batch 370/2351, Loss: 0.1491\n",
      "Epoch 49/50, Batch 380/2351, Loss: 0.1395\n",
      "Epoch 49/50, Batch 390/2351, Loss: 0.2235\n",
      "Epoch 49/50, Batch 400/2351, Loss: 0.3417\n",
      "Epoch 49/50, Batch 410/2351, Loss: 0.2904\n",
      "Epoch 49/50, Batch 420/2351, Loss: 0.0188\n",
      "Epoch 49/50, Batch 430/2351, Loss: 0.2431\n",
      "Epoch 49/50, Batch 440/2351, Loss: 0.4260\n",
      "Epoch 49/50, Batch 450/2351, Loss: 0.2658\n",
      "Epoch 49/50, Batch 460/2351, Loss: 0.2434\n",
      "Epoch 49/50, Batch 470/2351, Loss: 0.0642\n",
      "Epoch 49/50, Batch 480/2351, Loss: 0.1172\n",
      "Epoch 49/50, Batch 490/2351, Loss: 0.1630\n",
      "Epoch 49/50, Batch 500/2351, Loss: 0.1233\n",
      "Epoch 49/50, Batch 510/2351, Loss: 0.8922\n",
      "Epoch 49/50, Batch 520/2351, Loss: 0.1971\n",
      "Epoch 49/50, Batch 530/2351, Loss: 0.0778\n",
      "Epoch 49/50, Batch 540/2351, Loss: 0.0201\n",
      "Epoch 49/50, Batch 550/2351, Loss: 0.1670\n",
      "Epoch 49/50, Batch 560/2351, Loss: 0.2467\n",
      "Epoch 49/50, Batch 570/2351, Loss: 0.1648\n",
      "Epoch 49/50, Batch 580/2351, Loss: 0.0793\n",
      "Epoch 49/50, Batch 590/2351, Loss: 0.1530\n",
      "Epoch 49/50, Batch 600/2351, Loss: 0.1022\n",
      "Epoch 49/50, Batch 610/2351, Loss: 0.0657\n",
      "Epoch 49/50, Batch 620/2351, Loss: 0.1917\n",
      "Epoch 49/50, Batch 630/2351, Loss: 0.0278\n",
      "Epoch 49/50, Batch 640/2351, Loss: 0.0971\n",
      "Epoch 49/50, Batch 650/2351, Loss: 0.2365\n",
      "Epoch 49/50, Batch 660/2351, Loss: 0.3852\n",
      "Epoch 49/50, Batch 670/2351, Loss: 0.1333\n",
      "Epoch 49/50, Batch 680/2351, Loss: 0.2134\n",
      "Epoch 49/50, Batch 690/2351, Loss: 0.6909\n",
      "Epoch 49/50, Batch 700/2351, Loss: 0.1810\n",
      "Epoch 49/50, Batch 710/2351, Loss: 0.2227\n",
      "Epoch 49/50, Batch 720/2351, Loss: 0.4760\n",
      "Epoch 49/50, Batch 730/2351, Loss: 0.0834\n",
      "Epoch 49/50, Batch 740/2351, Loss: 0.0893\n",
      "Epoch 49/50, Batch 750/2351, Loss: 0.1077\n",
      "Epoch 49/50, Batch 760/2351, Loss: 0.0900\n",
      "Epoch 49/50, Batch 770/2351, Loss: 0.2370\n",
      "Epoch 49/50, Batch 780/2351, Loss: 0.1579\n",
      "Epoch 49/50, Batch 790/2351, Loss: 0.0110\n",
      "Epoch 49/50, Batch 800/2351, Loss: 0.1184\n",
      "Epoch 49/50, Batch 810/2351, Loss: 0.2018\n",
      "Epoch 49/50, Batch 820/2351, Loss: 0.1020\n",
      "Epoch 49/50, Batch 830/2351, Loss: 0.0660\n",
      "Epoch 49/50, Batch 840/2351, Loss: 0.1169\n",
      "Epoch 49/50, Batch 850/2351, Loss: 0.1963\n",
      "Epoch 49/50, Batch 860/2351, Loss: 0.3615\n",
      "Epoch 49/50, Batch 870/2351, Loss: 0.1225\n",
      "Epoch 49/50, Batch 880/2351, Loss: 0.0879\n",
      "Epoch 49/50, Batch 890/2351, Loss: 0.1116\n",
      "Epoch 49/50, Batch 900/2351, Loss: 0.3055\n",
      "Epoch 49/50, Batch 910/2351, Loss: 0.0081\n",
      "Epoch 49/50, Batch 920/2351, Loss: 0.3537\n",
      "Epoch 49/50, Batch 930/2351, Loss: 0.0910\n",
      "Epoch 49/50, Batch 940/2351, Loss: 0.2810\n",
      "Epoch 49/50, Batch 950/2351, Loss: 0.5014\n",
      "Epoch 49/50, Batch 960/2351, Loss: 0.1135\n",
      "Epoch 49/50, Batch 970/2351, Loss: 0.2692\n",
      "Epoch 49/50, Batch 980/2351, Loss: 0.5465\n",
      "Epoch 49/50, Batch 990/2351, Loss: 0.4011\n",
      "Epoch 49/50, Batch 1000/2351, Loss: 0.0960\n",
      "Epoch 49/50, Batch 1010/2351, Loss: 0.1646\n",
      "Epoch 49/50, Batch 1020/2351, Loss: 0.0240\n",
      "Epoch 49/50, Batch 1030/2351, Loss: 0.0149\n",
      "Epoch 49/50, Batch 1040/2351, Loss: 0.1724\n",
      "Epoch 49/50, Batch 1050/2351, Loss: 0.1107\n",
      "Epoch 49/50, Batch 1060/2351, Loss: 0.2839\n",
      "Epoch 49/50, Batch 1070/2351, Loss: 0.3831\n",
      "Epoch 49/50, Batch 1080/2351, Loss: 0.1741\n",
      "Epoch 49/50, Batch 1090/2351, Loss: 0.0422\n",
      "Epoch 49/50, Batch 1100/2351, Loss: 0.0490\n",
      "Epoch 49/50, Batch 1110/2351, Loss: 0.1259\n",
      "Epoch 49/50, Batch 1120/2351, Loss: 0.1136\n",
      "Epoch 49/50, Batch 1130/2351, Loss: 0.3058\n",
      "Epoch 49/50, Batch 1140/2351, Loss: 0.1237\n",
      "Epoch 49/50, Batch 1150/2351, Loss: 0.8820\n",
      "Epoch 49/50, Batch 1160/2351, Loss: 0.1382\n",
      "Epoch 49/50, Batch 1170/2351, Loss: 0.0328\n",
      "Epoch 49/50, Batch 1180/2351, Loss: 0.5307\n",
      "Epoch 49/50, Batch 1190/2351, Loss: 0.1151\n",
      "Epoch 49/50, Batch 1200/2351, Loss: 0.2647\n",
      "Epoch 49/50, Batch 1210/2351, Loss: 0.1839\n",
      "Epoch 49/50, Batch 1220/2351, Loss: 0.3234\n",
      "Epoch 49/50, Batch 1230/2351, Loss: 0.1059\n",
      "Epoch 49/50, Batch 1240/2351, Loss: 0.1487\n",
      "Epoch 49/50, Batch 1250/2351, Loss: 0.2265\n",
      "Epoch 49/50, Batch 1260/2351, Loss: 0.0105\n",
      "Epoch 49/50, Batch 1270/2351, Loss: 0.1612\n",
      "Epoch 49/50, Batch 1280/2351, Loss: 0.0196\n",
      "Epoch 49/50, Batch 1290/2351, Loss: 0.1488\n",
      "Epoch 49/50, Batch 1300/2351, Loss: 0.1566\n",
      "Epoch 49/50, Batch 1310/2351, Loss: 0.3776\n",
      "Epoch 49/50, Batch 1320/2351, Loss: 0.1197\n",
      "Epoch 49/50, Batch 1330/2351, Loss: 0.2370\n",
      "Epoch 49/50, Batch 1340/2351, Loss: 0.4478\n",
      "Epoch 49/50, Batch 1350/2351, Loss: 0.0141\n",
      "Epoch 49/50, Batch 1360/2351, Loss: 0.1167\n",
      "Epoch 49/50, Batch 1370/2351, Loss: 0.0669\n",
      "Epoch 49/50, Batch 1380/2351, Loss: 0.1552\n",
      "Epoch 49/50, Batch 1390/2351, Loss: 0.1896\n",
      "Epoch 49/50, Batch 1400/2351, Loss: 0.1712\n",
      "Epoch 49/50, Batch 1410/2351, Loss: 0.1392\n",
      "Epoch 49/50, Batch 1420/2351, Loss: 0.0434\n",
      "Epoch 49/50, Batch 1430/2351, Loss: 0.3050\n",
      "Epoch 49/50, Batch 1440/2351, Loss: 0.3916\n",
      "Epoch 49/50, Batch 1450/2351, Loss: 0.2000\n",
      "Epoch 49/50, Batch 1460/2351, Loss: 0.2040\n",
      "Epoch 49/50, Batch 1470/2351, Loss: 0.0361\n",
      "Epoch 49/50, Batch 1480/2351, Loss: 0.1422\n",
      "Epoch 49/50, Batch 1490/2351, Loss: 0.0326\n",
      "Epoch 49/50, Batch 1500/2351, Loss: 0.3542\n",
      "Epoch 49/50, Batch 1510/2351, Loss: 0.1165\n",
      "Epoch 49/50, Batch 1520/2351, Loss: 0.2271\n",
      "Epoch 49/50, Batch 1530/2351, Loss: 0.1869\n",
      "Epoch 49/50, Batch 1540/2351, Loss: 0.6407\n",
      "Epoch 49/50, Batch 1550/2351, Loss: 0.2238\n",
      "Epoch 49/50, Batch 1560/2351, Loss: 0.2615\n",
      "Epoch 49/50, Batch 1570/2351, Loss: 0.1242\n",
      "Epoch 49/50, Batch 1580/2351, Loss: 0.3228\n",
      "Epoch 49/50, Batch 1590/2351, Loss: 0.1955\n",
      "Epoch 49/50, Batch 1600/2351, Loss: 0.2093\n",
      "Epoch 49/50, Batch 1610/2351, Loss: 0.0298\n",
      "Epoch 49/50, Batch 1620/2351, Loss: 0.0298\n",
      "Epoch 49/50, Batch 1630/2351, Loss: 0.2515\n",
      "Epoch 49/50, Batch 1640/2351, Loss: 0.1342\n",
      "Epoch 49/50, Batch 1650/2351, Loss: 0.0093\n",
      "Epoch 49/50, Batch 1660/2351, Loss: 0.7475\n",
      "Epoch 49/50, Batch 1670/2351, Loss: 0.2844\n",
      "Epoch 49/50, Batch 1680/2351, Loss: 0.2236\n",
      "Epoch 49/50, Batch 1690/2351, Loss: 0.4059\n",
      "Epoch 49/50, Batch 1700/2351, Loss: 0.2465\n",
      "Epoch 49/50, Batch 1710/2351, Loss: 0.2108\n",
      "Epoch 49/50, Batch 1720/2351, Loss: 0.1592\n",
      "Epoch 49/50, Batch 1730/2351, Loss: 0.1824\n",
      "Epoch 49/50, Batch 1740/2351, Loss: 0.4666\n",
      "Epoch 49/50, Batch 1750/2351, Loss: 0.1880\n",
      "Epoch 49/50, Batch 1760/2351, Loss: 0.2194\n",
      "Epoch 49/50, Batch 1770/2351, Loss: 0.0977\n",
      "Epoch 49/50, Batch 1780/2351, Loss: 0.1905\n",
      "Epoch 49/50, Batch 1790/2351, Loss: 0.2058\n",
      "Epoch 49/50, Batch 1800/2351, Loss: 0.2310\n",
      "Epoch 49/50, Batch 1810/2351, Loss: 0.0498\n",
      "Epoch 49/50, Batch 1820/2351, Loss: 0.6983\n",
      "Epoch 49/50, Batch 1830/2351, Loss: 0.2562\n",
      "Epoch 49/50, Batch 1840/2351, Loss: 0.2338\n",
      "Epoch 49/50, Batch 1850/2351, Loss: 0.2310\n",
      "Epoch 49/50, Batch 1860/2351, Loss: 0.3015\n",
      "Epoch 49/50, Batch 1870/2351, Loss: 0.0831\n",
      "Epoch 49/50, Batch 1880/2351, Loss: 0.1131\n",
      "Epoch 49/50, Batch 1890/2351, Loss: 0.5175\n",
      "Epoch 49/50, Batch 1900/2351, Loss: 0.2096\n",
      "Epoch 49/50, Batch 1910/2351, Loss: 0.1236\n",
      "Epoch 49/50, Batch 1920/2351, Loss: 0.1835\n",
      "Epoch 49/50, Batch 1930/2351, Loss: 0.4641\n",
      "Epoch 49/50, Batch 1940/2351, Loss: 0.7369\n",
      "Epoch 49/50, Batch 1950/2351, Loss: 0.1116\n",
      "Epoch 49/50, Batch 1960/2351, Loss: 0.0402\n",
      "Epoch 49/50, Batch 1970/2351, Loss: 0.0086\n",
      "Epoch 49/50, Batch 1980/2351, Loss: 0.2146\n",
      "Epoch 49/50, Batch 1990/2351, Loss: 0.3566\n",
      "Epoch 49/50, Batch 2000/2351, Loss: 0.4142\n",
      "Epoch 49/50, Batch 2010/2351, Loss: 0.0550\n",
      "Epoch 49/50, Batch 2020/2351, Loss: 0.1835\n",
      "Epoch 49/50, Batch 2030/2351, Loss: 0.1674\n",
      "Epoch 49/50, Batch 2040/2351, Loss: 0.1978\n",
      "Epoch 49/50, Batch 2050/2351, Loss: 0.0724\n",
      "Epoch 49/50, Batch 2060/2351, Loss: 0.1053\n",
      "Epoch 49/50, Batch 2070/2351, Loss: 0.1432\n",
      "Epoch 49/50, Batch 2080/2351, Loss: 0.1349\n",
      "Epoch 49/50, Batch 2090/2351, Loss: 0.4424\n",
      "Epoch 49/50, Batch 2100/2351, Loss: 0.1987\n",
      "Epoch 49/50, Batch 2110/2351, Loss: 0.0588\n",
      "Epoch 49/50, Batch 2120/2351, Loss: 0.2615\n",
      "Epoch 49/50, Batch 2130/2351, Loss: 0.0304\n",
      "Epoch 49/50, Batch 2140/2351, Loss: 0.2818\n",
      "Epoch 49/50, Batch 2150/2351, Loss: 0.0473\n",
      "Epoch 49/50, Batch 2160/2351, Loss: 0.2137\n",
      "Epoch 49/50, Batch 2170/2351, Loss: 0.5113\n",
      "Epoch 49/50, Batch 2180/2351, Loss: 0.2302\n",
      "Epoch 49/50, Batch 2190/2351, Loss: 0.2864\n",
      "Epoch 49/50, Batch 2200/2351, Loss: 0.6798\n",
      "Epoch 49/50, Batch 2210/2351, Loss: 0.3038\n",
      "Epoch 49/50, Batch 2220/2351, Loss: 0.2428\n",
      "Epoch 49/50, Batch 2230/2351, Loss: 0.0363\n",
      "Epoch 49/50, Batch 2240/2351, Loss: 0.1774\n",
      "Epoch 49/50, Batch 2250/2351, Loss: 0.2899\n",
      "Epoch 49/50, Batch 2260/2351, Loss: 0.2475\n",
      "Epoch 49/50, Batch 2270/2351, Loss: 0.1359\n",
      "Epoch 49/50, Batch 2280/2351, Loss: 0.0652\n",
      "Epoch 49/50, Batch 2290/2351, Loss: 0.2548\n",
      "Epoch 49/50, Batch 2300/2351, Loss: 0.1618\n",
      "Epoch 49/50, Batch 2310/2351, Loss: 0.4570\n",
      "Epoch 49/50, Batch 2320/2351, Loss: 0.1951\n",
      "Epoch 49/50, Batch 2330/2351, Loss: 0.1755\n",
      "Epoch 49/50, Batch 2340/2351, Loss: 0.1933\n",
      "Epoch 49/50, Batch 2350/2351, Loss: 0.6523\n",
      "Epoch 49/50, Loss: 0.2066, Accuracy: 0.9275\n",
      "Checkpoint saved at epoch 49\n",
      "Validation Loss: 0.3015, Accuracy: 0.9048\n",
      "Specificity:  0.9809601672890601\n",
      "Recall:  0.9048475080585172\n",
      "Balanced Accuracy:  0.9429038376737886\n",
      "auc : 0.8614, acc : 0.9048, precision : 0.9024, recall : 0.9048, f1_score : 0.9023, specificity : 0.9810, balanced_acc : 0.9429\n",
      "------------------------------------ gt vs. pred ------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6622    0.8008    0.7250       497\n",
      "           1     0.8885    0.7772    0.8291       359\n",
      "           2     0.6436    0.6615    0.6524      1155\n",
      "           3     0.5690    0.3333    0.4204       297\n",
      "           4     0.9401    0.7382    0.8270       340\n",
      "           5     0.8658    0.7522    0.8050       343\n",
      "           6     0.9627    0.9761    0.9693     12287\n",
      "           7     0.4843    0.4320    0.4567       500\n",
      "           8     0.8799    0.9476    0.9125       286\n",
      "           9     0.8608    1.0000    0.9252        68\n",
      "\n",
      "    accuracy                         0.9048     16132\n",
      "   macro avg     0.7757    0.7419    0.7523     16132\n",
      "weighted avg     0.9024    0.9048    0.9023     16132\n",
      "\n",
      "[[  398     2    53     5     0     1    30     8     0     0]\n",
      " [   29   279    31     2     0     0     3    15     0     0]\n",
      " [  103    16   764    28     1    12   154    58    19     0]\n",
      " [   12     4   114    99     0     1    39    27     1     0]\n",
      " [    7     0    27     1   251     3    37    13     1     0]\n",
      " [    3     0    11     3     1   258    50    17     0     0]\n",
      " [   32     5   110    10    14    15 11993    92    14     2]\n",
      " [   17     8    77    26     0     8   146   216     2     0]\n",
      " [    0     0     0     0     0     0     6     0   271     9]\n",
      " [    0     0     0     0     0     0     0     0     0    68]]\n",
      "-------------------------------------------------------------------------------------\n",
      "Accuracy:  0.9048475080585172\n",
      "Epoch 50/50, Batch 0/2351, Loss: 0.1424\n",
      "Epoch 50/50, Batch 10/2351, Loss: 0.3633\n",
      "Epoch 50/50, Batch 20/2351, Loss: 0.1067\n",
      "Epoch 50/50, Batch 30/2351, Loss: 0.1568\n",
      "Epoch 50/50, Batch 40/2351, Loss: 0.2773\n",
      "Epoch 50/50, Batch 50/2351, Loss: 0.5463\n",
      "Epoch 50/50, Batch 60/2351, Loss: 0.0509\n",
      "Epoch 50/50, Batch 70/2351, Loss: 0.0075\n",
      "Epoch 50/50, Batch 80/2351, Loss: 0.0190\n",
      "Epoch 50/50, Batch 90/2351, Loss: 0.3562\n",
      "Epoch 50/50, Batch 100/2351, Loss: 0.0074\n",
      "Epoch 50/50, Batch 110/2351, Loss: 0.2638\n",
      "Epoch 50/50, Batch 120/2351, Loss: 0.0795\n",
      "Epoch 50/50, Batch 130/2351, Loss: 0.4727\n",
      "Epoch 50/50, Batch 140/2351, Loss: 0.0229\n",
      "Epoch 50/50, Batch 150/2351, Loss: 0.3781\n",
      "Epoch 50/50, Batch 160/2351, Loss: 0.1572\n",
      "Epoch 50/50, Batch 170/2351, Loss: 0.3581\n",
      "Epoch 50/50, Batch 180/2351, Loss: 0.1362\n",
      "Epoch 50/50, Batch 190/2351, Loss: 0.2184\n",
      "Epoch 50/50, Batch 200/2351, Loss: 0.4950\n",
      "Epoch 50/50, Batch 210/2351, Loss: 0.1976\n",
      "Epoch 50/50, Batch 220/2351, Loss: 0.2472\n",
      "Epoch 50/50, Batch 230/2351, Loss: 0.3425\n",
      "Epoch 50/50, Batch 240/2351, Loss: 0.8602\n",
      "Epoch 50/50, Batch 250/2351, Loss: 0.3591\n",
      "Epoch 50/50, Batch 260/2351, Loss: 0.1839\n",
      "Epoch 50/50, Batch 270/2351, Loss: 0.0154\n",
      "Epoch 50/50, Batch 280/2351, Loss: 0.5522\n",
      "Epoch 50/50, Batch 290/2351, Loss: 0.0501\n",
      "Epoch 50/50, Batch 300/2351, Loss: 0.0607\n",
      "Epoch 50/50, Batch 310/2351, Loss: 0.3552\n",
      "Epoch 50/50, Batch 320/2351, Loss: 0.1967\n",
      "Epoch 50/50, Batch 330/2351, Loss: 0.0653\n",
      "Epoch 50/50, Batch 340/2351, Loss: 0.0662\n",
      "Epoch 50/50, Batch 350/2351, Loss: 0.1934\n",
      "Epoch 50/50, Batch 360/2351, Loss: 0.0107\n",
      "Epoch 50/50, Batch 370/2351, Loss: 0.0050\n",
      "Epoch 50/50, Batch 380/2351, Loss: 0.1527\n",
      "Epoch 50/50, Batch 390/2351, Loss: 0.1794\n",
      "Epoch 50/50, Batch 400/2351, Loss: 0.1310\n",
      "Epoch 50/50, Batch 410/2351, Loss: 0.0723\n",
      "Epoch 50/50, Batch 420/2351, Loss: 0.0532\n",
      "Epoch 50/50, Batch 430/2351, Loss: 0.1819\n",
      "Epoch 50/50, Batch 440/2351, Loss: 0.0724\n",
      "Epoch 50/50, Batch 450/2351, Loss: 0.3346\n",
      "Epoch 50/50, Batch 460/2351, Loss: 0.1529\n",
      "Epoch 50/50, Batch 470/2351, Loss: 0.0022\n",
      "Epoch 50/50, Batch 480/2351, Loss: 0.2434\n",
      "Epoch 50/50, Batch 490/2351, Loss: 0.1073\n",
      "Epoch 50/50, Batch 500/2351, Loss: 0.0075\n",
      "Epoch 50/50, Batch 510/2351, Loss: 0.0263\n",
      "Epoch 50/50, Batch 520/2351, Loss: 0.1151\n",
      "Epoch 50/50, Batch 530/2351, Loss: 0.0337\n",
      "Epoch 50/50, Batch 540/2351, Loss: 0.4054\n",
      "Epoch 50/50, Batch 550/2351, Loss: 0.5542\n",
      "Epoch 50/50, Batch 560/2351, Loss: 0.4934\n",
      "Epoch 50/50, Batch 570/2351, Loss: 0.4093\n",
      "Epoch 50/50, Batch 580/2351, Loss: 0.1146\n",
      "Epoch 50/50, Batch 590/2351, Loss: 0.0335\n",
      "Epoch 50/50, Batch 600/2351, Loss: 0.1679\n",
      "Epoch 50/50, Batch 610/2351, Loss: 0.1614\n",
      "Epoch 50/50, Batch 620/2351, Loss: 0.4656\n",
      "Epoch 50/50, Batch 630/2351, Loss: 0.0767\n",
      "Epoch 50/50, Batch 640/2351, Loss: 0.2666\n",
      "Epoch 50/50, Batch 650/2351, Loss: 0.0811\n",
      "Epoch 50/50, Batch 660/2351, Loss: 0.1698\n",
      "Epoch 50/50, Batch 670/2351, Loss: 0.2508\n",
      "Epoch 50/50, Batch 680/2351, Loss: 0.0953\n",
      "Epoch 50/50, Batch 690/2351, Loss: 0.0560\n",
      "Epoch 50/50, Batch 700/2351, Loss: 0.2115\n",
      "Epoch 50/50, Batch 710/2351, Loss: 0.0494\n",
      "Epoch 50/50, Batch 720/2351, Loss: 0.0822\n",
      "Epoch 50/50, Batch 730/2351, Loss: 0.1001\n",
      "Epoch 50/50, Batch 740/2351, Loss: 0.2219\n",
      "Epoch 50/50, Batch 750/2351, Loss: 0.2438\n",
      "Epoch 50/50, Batch 760/2351, Loss: 0.0539\n",
      "Epoch 50/50, Batch 770/2351, Loss: 0.0634\n",
      "Epoch 50/50, Batch 780/2351, Loss: 0.0270\n",
      "Epoch 50/50, Batch 790/2351, Loss: 0.3198\n",
      "Epoch 50/50, Batch 800/2351, Loss: 0.3477\n",
      "Epoch 50/50, Batch 810/2351, Loss: 0.0522\n",
      "Epoch 50/50, Batch 820/2351, Loss: 0.0019\n",
      "Epoch 50/50, Batch 830/2351, Loss: 0.3695\n",
      "Epoch 50/50, Batch 840/2351, Loss: 0.0383\n",
      "Epoch 50/50, Batch 850/2351, Loss: 0.0060\n",
      "Epoch 50/50, Batch 860/2351, Loss: 0.4387\n",
      "Epoch 50/50, Batch 870/2351, Loss: 0.2492\n",
      "Epoch 50/50, Batch 880/2351, Loss: 0.3028\n",
      "Epoch 50/50, Batch 890/2351, Loss: 0.3549\n",
      "Epoch 50/50, Batch 900/2351, Loss: 0.1883\n",
      "Epoch 50/50, Batch 910/2351, Loss: 0.1995\n",
      "Epoch 50/50, Batch 920/2351, Loss: 0.1092\n",
      "Epoch 50/50, Batch 930/2351, Loss: 0.1315\n",
      "Epoch 50/50, Batch 940/2351, Loss: 0.2380\n",
      "Epoch 50/50, Batch 950/2351, Loss: 0.3754\n",
      "Epoch 50/50, Batch 960/2351, Loss: 0.5708\n",
      "Epoch 50/50, Batch 970/2351, Loss: 0.3382\n",
      "Epoch 50/50, Batch 980/2351, Loss: 0.0859\n",
      "Epoch 50/50, Batch 990/2351, Loss: 0.0756\n",
      "Epoch 50/50, Batch 1000/2351, Loss: 0.0945\n",
      "Epoch 50/50, Batch 1010/2351, Loss: 0.2403\n",
      "Epoch 50/50, Batch 1020/2351, Loss: 0.0835\n",
      "Epoch 50/50, Batch 1030/2351, Loss: 0.3290\n",
      "Epoch 50/50, Batch 1040/2351, Loss: 0.0560\n",
      "Epoch 50/50, Batch 1050/2351, Loss: 0.0587\n",
      "Epoch 50/50, Batch 1060/2351, Loss: 0.1437\n",
      "Epoch 50/50, Batch 1070/2351, Loss: 0.1651\n",
      "Epoch 50/50, Batch 1080/2351, Loss: 0.3168\n",
      "Epoch 50/50, Batch 1090/2351, Loss: 0.0859\n",
      "Epoch 50/50, Batch 1100/2351, Loss: 0.1470\n",
      "Epoch 50/50, Batch 1110/2351, Loss: 0.2674\n",
      "Epoch 50/50, Batch 1120/2351, Loss: 0.0566\n",
      "Epoch 50/50, Batch 1130/2351, Loss: 0.3200\n",
      "Epoch 50/50, Batch 1140/2351, Loss: 0.1696\n",
      "Epoch 50/50, Batch 1150/2351, Loss: 0.5374\n",
      "Epoch 50/50, Batch 1160/2351, Loss: 0.6048\n",
      "Epoch 50/50, Batch 1170/2351, Loss: 0.3202\n",
      "Epoch 50/50, Batch 1180/2351, Loss: 0.4457\n",
      "Epoch 50/50, Batch 1190/2351, Loss: 0.3558\n",
      "Epoch 50/50, Batch 1200/2351, Loss: 0.0890\n",
      "Epoch 50/50, Batch 1210/2351, Loss: 0.1367\n",
      "Epoch 50/50, Batch 1220/2351, Loss: 0.1981\n",
      "Epoch 50/50, Batch 1230/2351, Loss: 0.0443\n",
      "Epoch 50/50, Batch 1240/2351, Loss: 0.0014\n",
      "Epoch 50/50, Batch 1250/2351, Loss: 0.5638\n",
      "Epoch 50/50, Batch 1260/2351, Loss: 0.1731\n",
      "Epoch 50/50, Batch 1270/2351, Loss: 0.2089\n",
      "Epoch 50/50, Batch 1280/2351, Loss: 0.1400\n",
      "Epoch 50/50, Batch 1290/2351, Loss: 0.0662\n",
      "Epoch 50/50, Batch 1300/2351, Loss: 0.0635\n",
      "Epoch 50/50, Batch 1310/2351, Loss: 0.0532\n",
      "Epoch 50/50, Batch 1320/2351, Loss: 0.1957\n",
      "Epoch 50/50, Batch 1330/2351, Loss: 0.0186\n",
      "Epoch 50/50, Batch 1340/2351, Loss: 0.1802\n",
      "Epoch 50/50, Batch 1350/2351, Loss: 0.0161\n",
      "Epoch 50/50, Batch 1360/2351, Loss: 0.2243\n",
      "Epoch 50/50, Batch 1370/2351, Loss: 0.1480\n",
      "Epoch 50/50, Batch 1380/2351, Loss: 0.0791\n",
      "Epoch 50/50, Batch 1390/2351, Loss: 0.2634\n",
      "Epoch 50/50, Batch 1400/2351, Loss: 0.0789\n",
      "Epoch 50/50, Batch 1410/2351, Loss: 0.1578\n",
      "Epoch 50/50, Batch 1420/2351, Loss: 0.0424\n",
      "Epoch 50/50, Batch 1430/2351, Loss: 0.3477\n",
      "Epoch 50/50, Batch 1440/2351, Loss: 0.0476\n",
      "Epoch 50/50, Batch 1450/2351, Loss: 0.0630\n",
      "Epoch 50/50, Batch 1460/2351, Loss: 0.2405\n",
      "Epoch 50/50, Batch 1470/2351, Loss: 0.0347\n",
      "Epoch 50/50, Batch 1480/2351, Loss: 0.3444\n",
      "Epoch 50/50, Batch 1490/2351, Loss: 0.4039\n",
      "Epoch 50/50, Batch 1500/2351, Loss: 0.1839\n",
      "Epoch 50/50, Batch 1510/2351, Loss: 0.0836\n",
      "Epoch 50/50, Batch 1520/2351, Loss: 0.1194\n",
      "Epoch 50/50, Batch 1530/2351, Loss: 0.0842\n",
      "Epoch 50/50, Batch 1540/2351, Loss: 0.5012\n",
      "Epoch 50/50, Batch 1550/2351, Loss: 0.0485\n",
      "Epoch 50/50, Batch 1560/2351, Loss: 0.0454\n",
      "Epoch 50/50, Batch 1570/2351, Loss: 0.0638\n",
      "Epoch 50/50, Batch 1580/2351, Loss: 0.1188\n",
      "Epoch 50/50, Batch 1590/2351, Loss: 0.1841\n",
      "Epoch 50/50, Batch 1600/2351, Loss: 0.1389\n",
      "Epoch 50/50, Batch 1610/2351, Loss: 0.0663\n",
      "Epoch 50/50, Batch 1620/2351, Loss: 0.2911\n",
      "Epoch 50/50, Batch 1630/2351, Loss: 0.0303\n",
      "Epoch 50/50, Batch 1640/2351, Loss: 0.1543\n",
      "Epoch 50/50, Batch 1650/2351, Loss: 0.2944\n",
      "Epoch 50/50, Batch 1660/2351, Loss: 0.0930\n",
      "Epoch 50/50, Batch 1670/2351, Loss: 0.0118\n",
      "Epoch 50/50, Batch 1680/2351, Loss: 0.0510\n",
      "Epoch 50/50, Batch 1690/2351, Loss: 0.2308\n",
      "Epoch 50/50, Batch 1700/2351, Loss: 0.0267\n",
      "Epoch 50/50, Batch 1710/2351, Loss: 0.6504\n",
      "Epoch 50/50, Batch 1720/2351, Loss: 0.0480\n",
      "Epoch 50/50, Batch 1730/2351, Loss: 0.0507\n",
      "Epoch 50/50, Batch 1740/2351, Loss: 0.2584\n",
      "Epoch 50/50, Batch 1750/2351, Loss: 0.2582\n",
      "Epoch 50/50, Batch 1760/2351, Loss: 0.1525\n",
      "Epoch 50/50, Batch 1770/2351, Loss: 0.4621\n",
      "Epoch 50/50, Batch 1780/2351, Loss: 0.1066\n",
      "Epoch 50/50, Batch 1790/2351, Loss: 0.0511\n",
      "Epoch 50/50, Batch 1800/2351, Loss: 1.0977\n",
      "Epoch 50/50, Batch 1810/2351, Loss: 0.1743\n",
      "Epoch 50/50, Batch 1820/2351, Loss: 0.3105\n",
      "Epoch 50/50, Batch 1830/2351, Loss: 0.1771\n",
      "Epoch 50/50, Batch 1840/2351, Loss: 0.3230\n",
      "Epoch 50/50, Batch 1850/2351, Loss: 0.2533\n",
      "Epoch 50/50, Batch 1860/2351, Loss: 0.1376\n",
      "Epoch 50/50, Batch 1870/2351, Loss: 0.0408\n",
      "Epoch 50/50, Batch 1880/2351, Loss: 0.0802\n",
      "Epoch 50/50, Batch 1890/2351, Loss: 0.1607\n",
      "Epoch 50/50, Batch 1900/2351, Loss: 0.4285\n",
      "Epoch 50/50, Batch 1910/2351, Loss: 0.2248\n",
      "Epoch 50/50, Batch 1920/2351, Loss: 0.0703\n",
      "Epoch 50/50, Batch 1930/2351, Loss: 0.4692\n",
      "Epoch 50/50, Batch 1940/2351, Loss: 0.3027\n",
      "Epoch 50/50, Batch 1950/2351, Loss: 0.0088\n",
      "Epoch 50/50, Batch 1960/2351, Loss: 0.2259\n",
      "Epoch 50/50, Batch 1970/2351, Loss: 0.6552\n",
      "Epoch 50/50, Batch 1980/2351, Loss: 0.2129\n",
      "Epoch 50/50, Batch 1990/2351, Loss: 0.2300\n",
      "Epoch 50/50, Batch 2000/2351, Loss: 0.2283\n",
      "Epoch 50/50, Batch 2010/2351, Loss: 0.1433\n",
      "Epoch 50/50, Batch 2020/2351, Loss: 0.0509\n",
      "Epoch 50/50, Batch 2030/2351, Loss: 0.2796\n",
      "Epoch 50/50, Batch 2040/2351, Loss: 0.0346\n",
      "Epoch 50/50, Batch 2050/2351, Loss: 0.1588\n",
      "Epoch 50/50, Batch 2060/2351, Loss: 0.2625\n",
      "Epoch 50/50, Batch 2070/2351, Loss: 0.5223\n",
      "Epoch 50/50, Batch 2080/2351, Loss: 0.0287\n",
      "Epoch 50/50, Batch 2090/2351, Loss: 0.0438\n",
      "Epoch 50/50, Batch 2100/2351, Loss: 0.4165\n",
      "Epoch 50/50, Batch 2110/2351, Loss: 0.0711\n",
      "Epoch 50/50, Batch 2120/2351, Loss: 0.3925\n",
      "Epoch 50/50, Batch 2130/2351, Loss: 0.2803\n",
      "Epoch 50/50, Batch 2140/2351, Loss: 0.4132\n",
      "Epoch 50/50, Batch 2150/2351, Loss: 0.3236\n",
      "Epoch 50/50, Batch 2160/2351, Loss: 0.3652\n",
      "Epoch 50/50, Batch 2170/2351, Loss: 0.1524\n",
      "Epoch 50/50, Batch 2180/2351, Loss: 0.2010\n",
      "Epoch 50/50, Batch 2190/2351, Loss: 0.7794\n",
      "Epoch 50/50, Batch 2200/2351, Loss: 0.1611\n",
      "Epoch 50/50, Batch 2210/2351, Loss: 0.1526\n",
      "Epoch 50/50, Batch 2220/2351, Loss: 0.3573\n",
      "Epoch 50/50, Batch 2230/2351, Loss: 0.3684\n",
      "Epoch 50/50, Batch 2240/2351, Loss: 0.0975\n",
      "Epoch 50/50, Batch 2250/2351, Loss: 0.3938\n",
      "Epoch 50/50, Batch 2260/2351, Loss: 0.4619\n",
      "Epoch 50/50, Batch 2270/2351, Loss: 0.1273\n",
      "Epoch 50/50, Batch 2280/2351, Loss: 0.3097\n",
      "Epoch 50/50, Batch 2290/2351, Loss: 0.0976\n",
      "Epoch 50/50, Batch 2300/2351, Loss: 0.1558\n",
      "Epoch 50/50, Batch 2310/2351, Loss: 0.1137\n",
      "Epoch 50/50, Batch 2320/2351, Loss: 0.1127\n",
      "Epoch 50/50, Batch 2330/2351, Loss: 0.5974\n",
      "Epoch 50/50, Batch 2340/2351, Loss: 0.0890\n",
      "Epoch 50/50, Batch 2350/2351, Loss: 0.3743\n",
      "Epoch 50/50, Loss: 0.2058, Accuracy: 0.9277\n",
      "Checkpoint saved at epoch 50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import cv2\n",
    "import pywt\n",
    "random.seed(30)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, number_of_channels, num_classes, pretrained = True):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet34_4_channel = get_arch(18, number_of_channels)\n",
    "        model = resnet34_4_channel(pretrained)    \n",
    "        self.backbone = model\n",
    "        \n",
    "        self.oga1 = ODConv2d(64, 64, 3)\n",
    "        self.oga2 = ODConv2d(64, 64, 3)\n",
    "        self.oga3 = ODConv2d(128, 128, 3)\n",
    "        self.oga4 = ODConv2d(256, 256, 3)\n",
    "\n",
    "    def _freeze_model(self):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.required_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        y = self.oga1(x)\n",
    "        outs.append(y)\n",
    "\n",
    "        x = self.backbone.maxpool(x)\n",
    "        x = self.backbone.layer1(x)\n",
    "        y = self.oga2(x)\n",
    "        outs.append(y)\n",
    "\n",
    "        x = self.backbone.layer2(x)\n",
    "        y = self.oga3(x)\n",
    "        outs.append(y)\n",
    "\n",
    "        x = self.backbone.layer3(x)\n",
    "        y = self.oga4(x)\n",
    "        outs.append(y)\n",
    "\n",
    "        x = self.backbone.layer4(x)\n",
    "        outs.append(x)\n",
    "        return tuple(outs)\n",
    "\n",
    "def _make_res_layer(block, num_residual_blocks, in_channels, out_channels, stride=1):\n",
    "    identity_downsample = None\n",
    "    layers = []\n",
    "\n",
    "    if stride != 1 or in_channels != out_channels:\n",
    "        identity_downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    layers.append(block(in_channels, out_channels, identity_downsample, stride))\n",
    "    in_channels = out_channels\n",
    "    in_channels = out_channels\n",
    "\n",
    "    for i in range(num_residual_blocks - 1):\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "            \n",
    "# Decoder class definition\n",
    "class DeCoder(nn.Module):\n",
    "    def __init__(self, number_of_channels):\n",
    "        super(DeCoder, self).__init__()\n",
    "        self.in_channels = (64, 64, 129, 258, 516)\n",
    "        self.out_channels = (516, 258, 129, 64, 64, number_of_channels)\n",
    "        self.res_layers = []\n",
    "        self.conv1x1 = []\n",
    "        self.conv2x2 = []\n",
    "        self._make_layers()\n",
    "\n",
    "    def _make_layers(self):\n",
    "        for i in range(len(self.in_channels)-1, -1, -1):\n",
    "            res_layer = _make_res_layer(\n",
    "                            block=BasicBlock,\n",
    "                            num_residual_blocks=2,\n",
    "                            in_channels=128 if (i <= 2 and i!=0) else self.in_channels[i],\n",
    "                            out_channels=self.out_channels[-(i+1)],\n",
    "                            stride=1)\n",
    "            out_planes = self.in_channels[i] if i < 2 else int(self.in_channels[i]//2)\n",
    "            conv2x2 = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels[i], out_planes, kernel_size=2, bias=False),\n",
    "                nn.InstanceNorm2d(out_planes),\n",
    "                nn.ReLU(inplace=True))\n",
    "\n",
    "            conv1x1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=128 if (i <= 2 and i!=0) else self.in_channels[i],\n",
    "                          out_channels=self.out_channels[-(i+1)],\n",
    "                          kernel_size=1,\n",
    "                          bias=False),\n",
    "                nn.InstanceNorm2d(self.out_channels[-(i+1)]))\n",
    "            self.res_layers.append(res_layer)\n",
    "            self.conv2x2.append(conv2x2)\n",
    "            self.conv1x1.append(conv1x1)\n",
    "        self.res_layers = nn.ModuleList(self.res_layers)\n",
    "        self.conv2x2 = nn.ModuleList(self.conv2x2)\n",
    "        self.conv1x1 = nn.ModuleList(self.conv1x1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x) == len(self.in_channels)\n",
    "\n",
    "        out = x[-1]\n",
    "        outs = []\n",
    "        outs.append(out)\n",
    "\n",
    "        for i in range(len(self.in_channels)):\n",
    "            out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = F.pad(out, [0, 1, 0, 1])\n",
    "            out = self.conv2x2[i](out)\n",
    "            if i < 4:\n",
    "                out = torch.cat([out, x[-(i+2)]], dim=1)\n",
    "            identity = self.conv1x1[i](out)\n",
    "            out = self.res_layers[i](out) + identity\n",
    "            outs.append(out)\n",
    "        outs[-1] = torch.tanh(outs[-1])\n",
    "        return outs\n",
    "\n",
    "# BasicBlock and ResNet classes for building the ResNet model\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "resnet_models = {18: torchvision.models.resnet18,\n",
    "                 34: torchvision.models.resnet34,\n",
    "                 50: torchvision.models.resnet18,\n",
    "                 101: torchvision.models.resnet101,\n",
    "                 152: torchvision.models.resnet152}\n",
    "\n",
    "class Resnet_multichannel(nn.Module):\n",
    "    def __init__(self, pretrained=True, encoder_depth=18, num_in_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        if encoder_depth not in [18, 34, 50, 101, 152]:\n",
    "            raise ValueError(f\"Encoder depth {encoder_depth} specified does not match any existing Resnet models\")\n",
    "\n",
    "        model = resnet_models[encoder_depth](pretrained)\n",
    "\n",
    "        self.conv1 = self.increase_channels(model.conv1, num_in_channels)\n",
    "\n",
    "        self.bn1 = model.bn1\n",
    "        self.relu = model.relu\n",
    "        self.maxpool = model.maxpool\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "        self.avgpool = model.avgpool\n",
    "        self.fc = model.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def increase_channels(self, m, num_channels=None, copy_weights=0):\n",
    "        new_in_channels = num_channels if num_channels is not None else m.in_channels + 1\n",
    "        bias = False if m.bias is None else True\n",
    "\n",
    "        # Creating new Conv2d layer\n",
    "        new_m = nn.Conv2d(in_channels=new_in_channels,\n",
    "                          out_channels=m.out_channels,\n",
    "                          kernel_size=m.kernel_size,\n",
    "                          stride=m.stride,\n",
    "                          padding=m.padding,\n",
    "                          bias=bias)\n",
    "\n",
    "        # Copying the weights from the old to the new layer\n",
    "        new_m.weight[:, :m.in_channels, :, :].data[:, :m.in_channels, :, :] = m.weight.clone()\n",
    "\n",
    "        #Copying the weights of the `copy_weights` channel of the old layer to the extra channels of the new layer\n",
    "        for i in range(new_in_channels - m.in_channels):\n",
    "            channel = m.in_channels + i\n",
    "            new_m.weight[:, channel:channel+1, :, :].data[:, channel:channel+1, :, :] = m.weight[:, copy_weights:copy_weights+1, : :].clone()\n",
    "        new_m.weight = nn.Parameter(new_m.weight)\n",
    "\n",
    "        return new_m\n",
    "\n",
    "def get_arch(encoder_depth, num_in_channels):\n",
    "    return partial(Resnet_multichannel, encoder_depth=encoder_depth, num_in_channels=num_in_channels)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes = 10, drop_ratio = 0.5, number_of_channels=3):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "        resnet34_4_channel = get_arch(18, number_of_channels)\n",
    "\n",
    "        # use resnet34_4_channels(False) to get a non pretrained model\n",
    "        model = resnet34_4_channel(True)\n",
    "        self.resnet18 = model\n",
    "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18.conv1(x)\n",
    "        x = self.resnet18.bn1(x)\n",
    "        x = self.resnet18.relu(x)\n",
    "        x = self.resnet18.layer1(x)\n",
    "        x = self.resnet18.layer2(x)\n",
    "        x = self.resnet18.layer3(x)\n",
    "        x = self.resnet18.layer4(x)\n",
    "        x = self.resnet18.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.resnet18.fc(x)\n",
    "        return x\n",
    "\n",
    "# Omni-dimensional Gated Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, groups=1, reduction=0.0625, kernel_num=4, min_channel=16):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # Attention channel size is determined by the reduction ratio\n",
    "        attention_channel = max(int(in_planes * reduction), min_channel)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_num = kernel_num\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(attention_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Channel attention\n",
    "        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)\n",
    "        self.func_channel = self.get_channel_attention\n",
    "\n",
    "        # Filter attention (optional for depth-wise convolution)\n",
    "        if in_planes == groups and in_planes == out_planes:  \n",
    "            self.func_filter = self.skip\n",
    "        else:\n",
    "            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)\n",
    "            self.func_filter = self.get_filter_attention\n",
    "\n",
    "        # Spatial attention (if kernel size > 1)\n",
    "        if kernel_size == 1:  \n",
    "            self.func_spatial = self.skip\n",
    "        else:\n",
    "            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)\n",
    "            self.func_spatial = self.get_spatial_attention\n",
    "\n",
    "        # Kernel attention (optional if only one kernel is used)\n",
    "        if kernel_num == 1:\n",
    "            self.func_kernel = self.skip\n",
    "        else:\n",
    "            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)\n",
    "            self.func_kernel = self.get_kernel_attention\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def skip(_):\n",
    "        return 1.0\n",
    "\n",
    "    # Channel attention computation\n",
    "    def get_channel_attention(self, x):\n",
    "        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return channel_attention\n",
    "\n",
    "    # Filter attention computation\n",
    "    def get_filter_attention(self, x):\n",
    "        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return filter_attention\n",
    "\n",
    "    # Spatial attention computation\n",
    "    def get_spatial_attention(self, x):\n",
    "        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)\n",
    "        return spatial_attention\n",
    "\n",
    "    # Kernel attention computation\n",
    "    def get_kernel_attention(self, x):\n",
    "        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)\n",
    "        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)\n",
    "        return kernel_attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)\n",
    "\n",
    "\n",
    "# ODConv2d Layer with Omni-dimensional Attention\n",
    "class ODConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=1, dilation=1, groups=1,\n",
    "                 reduction=0.0625, kernel_num=4):\n",
    "        super(ODConv2d, self).__init__()\n",
    "        \n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.kernel_num = kernel_num\n",
    "        \n",
    "        # Attention mechanism for omni-dimensional convolution\n",
    "        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,\n",
    "                                   reduction=reduction, kernel_num=kernel_num)\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # Define different forward methods based on kernel size and number\n",
    "        if self.kernel_size == 1 and self.kernel_num == 1:\n",
    "            self._forward_impl = self._forward_impl_pw1x\n",
    "        else:\n",
    "            self._forward_impl = self._forward_impl_common\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for i in range(self.kernel_num):\n",
    "            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.attention.update_temperature(temperature)\n",
    "\n",
    "    # Standard forward method for dynamic kernels\n",
    "    def _forward_impl_common(self, x):\n",
    "        # Apply attention across all dimensions\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        \n",
    "        batch_size, in_planes, height, width = x.size()\n",
    "        x = x * channel_attention\n",
    "        x = x.reshape(1, -1, height, width)\n",
    "\n",
    "        # Calculate the aggregate weight based on attention\n",
    "        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)\n",
    "\n",
    "        # Summing weights for all kernels\n",
    "        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(\n",
    "            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])\n",
    "\n",
    "        # Convolution with aggregated weights\n",
    "        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups * batch_size)\n",
    "\n",
    "        # Reshape the output and apply filter attention\n",
    "        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))\n",
    "        output = output * filter_attention\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Point-wise convolution (1x1 kernel) forward method\n",
    "    def _forward_impl_pw1x(self, x):\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        \n",
    "        # Apply channel attention and perform standard convolution\n",
    "        x = x * channel_attention\n",
    "        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups)\n",
    "        \n",
    "        # Apply filter attention to the output\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    # Main forward method to dispatch the appropriate implementation\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "#Apply Stationary Wavelet Transformation    \n",
    "def apply_swt(image):\n",
    "    coeffs = pywt.swt2(image.cpu().numpy(), wavelet='haar', level=1, start_level=0, axes=(-2, -1))\n",
    "    cA, (cH, cV, cD) = coeffs[0]\n",
    "    swt_image = torch.cat([torch.tensor(cA), torch.tensor(cH), torch.tensor(cV), torch.tensor(cD)], dim=1)\n",
    "    return swt_image.to(image.device)\n",
    "\n",
    "#Apply Discrete Wavelet Transformation\n",
    "def apply_dwt(image):\n",
    "    \n",
    "    cA, (cH, cV, cD) = pywt.dwt2(image.cpu().numpy(), 'haar')\n",
    "    \n",
    "    dwt_image = torch.cat([torch.tensor(cA), torch.tensor(cH)], dim=1)\n",
    "    return dwt_image.to(image.device)\n",
    "\n",
    "#Apply GrayScale\n",
    "class ToGrayscale:\n",
    "    def __call__(self, tensor):\n",
    "        R, G, B = tensor[:, 0, :, :], tensor[:, 1, :, :], tensor[:, 2, :, :]\n",
    "        grayscale = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "        return grayscale.unsqueeze(1)\n",
    "\n",
    "# UNetResNet18 definition\n",
    "class UNetResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetResNet18, self).__init__()\n",
    "        self.encoder = Encoder(3, 10)\n",
    "        self.decoder = DeCoder(3)\n",
    "        self.resnet = Classifier(num_classes = 10, number_of_channels=3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out = self.encoder(x) #Encoder Output\n",
    "        \n",
    "        enc_outs = list(enc_out)\n",
    "        to_grayscale = ToGrayscale()\n",
    "        swt_image = apply_swt(to_grayscale(x)) #applies SWT\n",
    "        swt_images_resized = F.interpolate(swt_image, size=enc_outs[-1].shape[2:], mode='nearest')\n",
    "        \n",
    "        dwt_image = apply_dwt(to_grayscale(x)) #applies DWT\n",
    "        dwt_images_resized = F.interpolate(dwt_image, size=enc_outs[-2].shape[2:], mode='nearest')\n",
    "        \n",
    "        to_grayscale_resized2 = F.interpolate(to_grayscale(x), size=enc_outs[-3].shape[2:], mode='nearest')\n",
    "        \n",
    "        enc_outs[-1] = torch.cat([enc_outs[-1], swt_images_resized], dim=1)    # Concatenate SWT components (4 channels) to (512, 8, 8)\n",
    "        enc_outs[-2] = torch.cat([enc_outs[-2], dwt_images_resized], dim=1)    # Concatenate DWT components (2 channels) to (256, 16, 16)\n",
    "        enc_outs[-3] = torch.cat([enc_outs[-3], to_grayscale_resized2], dim=1) # Concatenate GrayScale\n",
    "        \n",
    "        dec_outs = self.decoder(enc_outs) #Decoder Output\n",
    "        \n",
    "        resnet_out = self.resnet(dec_outs[-1]) #Classifier\n",
    "        return resnet_out\n",
    "\n",
    "# Function to save a checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, filename='CP-PT_CVIP_2024.pth'):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Function to load a checkpoint\n",
    "def load_checkpoint(model, optimizer, filename='CP-PT_CVIP_2024.pth'):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch']\n",
    "    return 0\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_acc(gt_y, pred_y, result_file): \n",
    "    y_score = np.eye(len(np.unique(gt_y)))[pred_y]\n",
    "\n",
    "    auc = metrics.roc_auc_score(gt_y, y_score, multi_class='ovr')\n",
    "    acc = metrics.accuracy_score(gt_y, pred_y)\n",
    "    recall = metrics.recall_score(gt_y, pred_y, average='weighted')\n",
    "    f1_score = metrics.f1_score(gt_y, pred_y, average='weighted')\n",
    "    precision = metrics.precision_score(gt_y, pred_y, average='weighted')\n",
    "    \n",
    "    # Confusion matrix for all classes\n",
    "    cm = metrics.confusion_matrix(gt_y, pred_y)\n",
    "\n",
    "    # Specificity calculation for multiclass\n",
    "    specificity_list = []\n",
    "    for i in range(len(cm)):\n",
    "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i] \n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        specificity_list.append(specificity)\n",
    "    \n",
    "    # Average specificity across all classes\n",
    "    specificity = np.mean(specificity_list)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    \n",
    "    #Balanced Accuracy\n",
    "    balanced_acc = (recall + specificity) / 2\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Balanced Accuracy: \", balanced_acc)\n",
    "\n",
    "    print('auc : %.4f, acc : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f, specificity : %.4f, balanced_acc : %.4f' % \\\n",
    "          (auc, acc, precision, recall, f1_score, specificity, balanced_acc))\n",
    "    \n",
    "    auc_report = 'auc : %.4f, acc : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f, specificity : %.4f, balanced_acc : %.4f' % \\\n",
    "                 (auc, acc, precision, recall, f1_score, specificity, balanced_acc)\n",
    "\n",
    "    print('%s gt vs. pred %s' % ('-' * 36, '-' * 36))\n",
    "    print(metrics.classification_report(gt_y, pred_y, digits=4))\n",
    "    print(metrics.confusion_matrix(gt_y, pred_y))\n",
    "    print('-' * 85)\n",
    "    \n",
    "    result_file = result_file.replace(\".txt\", \".csv\")\n",
    "    report = metrics.classification_report(gt_y, pred_y, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(result_file, mode='a', index=True)\n",
    "    \n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(\"-------------------------------------------------------\\n\")\n",
    "        f.write(auc_report)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(np.array2string(metrics.confusion_matrix(gt_y, pred_y), separator=', '))\n",
    "        f.write(\"\\n-------------------------------------------------------\\n\")\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, checkpoint_file='CP-PT_CVIP_2024.pth'):\n",
    "    model.train()\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_file)\n",
    "    if start_epoch > 0:\n",
    "        print(f'Resuming training from epoch {start_epoch + 2}')\n",
    "        start_epoch=start_epoch+1\n",
    "    metrics = load_metrics_from_csv()\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:  # Print every 100 batches\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "        # Store training metrics\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        # Save the checkpoint after each epoch\n",
    "        save_checkpoint(epoch, model, optimizer, checkpoint_file)\n",
    "        print(f'Checkpoint saved at epoch {epoch + 1}')\n",
    "\n",
    "        val_loss, val_accuracy = validate_model(model, valid_loader, criterion)\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Append new metrics to the existing metrics\n",
    "        metrics['train_losses'].extend(train_losses)\n",
    "        metrics['valid_losses'].extend(valid_losses)\n",
    "        metrics['train_accuracies'].extend(train_accuracies)\n",
    "        metrics['valid_accuracies'].extend(valid_accuracies)\n",
    "\n",
    "        # Save updated metrics\n",
    "        save_metrics_to_csv(metrics)\n",
    "\n",
    "        # Plot the metrics\n",
    "        plot_metrics(metrics)\n",
    "        \n",
    "        # Clear the lists for the next epoch\n",
    "        train_losses.clear()\n",
    "        valid_losses.clear()\n",
    "        train_accuracies.clear()\n",
    "        valid_accuracies.clear()\n",
    "\n",
    "# Validation function with return of metrics\n",
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    originallabels = []\n",
    "    predictedlabels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            originallabels.extend(labels.cpu().numpy())\n",
    "            predictedlabels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(valid_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    result_file = \"PT_CVIP_2024.txt\"\n",
    "    acc = calculate_acc(originallabels, predictedlabels, result_file)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    return val_loss, accuracy\n",
    "\n",
    "# Function to plot training vs validation metrics and save the plot\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_metrics_to_csv(data, filename='metrics_data.csv'):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['epoch', 'train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy'])\n",
    "        for i in range(len(data['train_losses'])):\n",
    "            writer.writerow([i+1, data['train_losses'][i], data['valid_losses'][i], data['train_accuracies'][i], data['valid_accuracies'][i]])\n",
    "\n",
    "def load_metrics_from_csv(filename='metrics_data.csv'):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, mode='r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            data = {'train_losses': [], 'valid_losses': [], 'train_accuracies': [], 'valid_accuracies': []}\n",
    "            for row in reader:\n",
    "                data['train_losses'].append(float(row['train_loss']))\n",
    "                data['valid_losses'].append(float(row['valid_loss']))\n",
    "                data['train_accuracies'].append(float(row['train_accuracy']))\n",
    "                data['valid_accuracies'].append(float(row['valid_accuracy']))\n",
    "            return data\n",
    "    return {'train_losses': [], 'valid_losses': [], 'train_accuracies': [], 'valid_accuracies': []}\n",
    "\n",
    "def plot_metrics(metrics, filename='PLOT_PT_CVIP_2024.png'):\n",
    "    epochs = range(1, len(metrics['train_losses']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics['train_losses'], 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, metrics['valid_losses'], 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, metrics['train_accuracies'], 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, metrics['valid_accuracies'], 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Base Directory path\n",
    "    base_dir = \"/workspace/Dataset/Dataset\"\n",
    "\n",
    "    # Define the directories for train, validation\n",
    "    train_dir = os.path.join(base_dir, 'training')\n",
    "    valid_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "    #Transformation Pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "    ])  \n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\n",
    "    \n",
    "    #Printing Training and Validation dataset sizes\n",
    "    print('Train Size: ', len(train_dataset))\n",
    "    print('Valid Size: ', len(valid_dataset))\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNetResNet18().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    # Train and validate the model\n",
    "    num_epochs = 50\n",
    "    checkpoint_file = 'CP-PT_CVIP_2024.pth'\n",
    "    train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, checkpoint_file)\n",
    "    #validate_model(model, valid_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308f868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
